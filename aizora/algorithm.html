<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AIzora - Subset Selection Methods</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <header>
    <h1>Subset Selection Methods</h1>
  </header>
  <nav>
    <a href="index.html">Home</a>
    <a href="blog.html">Back to Blog</a>
    <a href="paper-summary.html">Paper Summaries</a>
  </nav>
  <main style="max-width: 800px; margin: 0 auto;">
    <h2>Subset Selection from Pretrained Embeddings</h2>
    <p><strong>Note:</strong> If you think I should correct something, let me know ðŸ˜Š.</p>
    <p>This blog includes:</p>
    <ol>
      <li><a href="#intro">Introduction</a></li>
      <li><a href="#craig">CRAIG Method</a></li>
      <li><a href="#pbc">Probabilistic Bilevel Coreset (PBC)</a></li>
      <li><a href="#stochastic-greedy">Stochastic Greedy Selection</a></li>
      <li><a href="#comparison">Comparison Table</a></li>
      <li><a href="#code">Subset Selection Code</a></li>
    </ol>

    <h3 id="intro">Introduction</h3>
    <p>Subset selection methods aim to select a small yet representative subset of a dataset, especially useful when training on large-scale embeddings. These techniques allow us to retain the model's performance while reducing compute and memory costs.</p>

    <h3 id="craig">CRAIG Method</h3>
    <p>CRAIG selects points with high influence on the objective defined by the trace of the kernel matrix: \( \text{Tr}(G_S^T G_S) \). This simplifies to choosing points with the highest L2 norm in the embedding space:</p>
    <p>
      \[
      \text{Select } S \subseteq D,\ \text{such that } \sum_{x_i \in S} \|G_i\|^2 \text{ is maximized}
      \]
    </p>
    <pre><code class="language-python">norms_sq = torch.sum(embeddings**2, dim=1)
selected_indices = torch.topk(norms_sq, k=subset_size).indices</code></pre>

    <h3 id="pbc">Probabilistic Bilevel Coreset (PBC)</h3>
    <p>PBC frames subset selection as a bilevel optimization problem. It selects a probability vector \( p \in \Delta_k \) such that training on a weighted dataset gives best performance on a held-out set:</p>
    <p>
      \[
      \min_{p \in \Delta_k} L_{val}(\theta^*(p)),\ \text{where } \theta^*(p) = \arg\min_\theta \sum_i p_i \ell(\theta; x_i)
      \]
    </p>
    <p>Due to its complexity, this method is often approximated. In our implementation, a random sampling placeholder is used for practical purposes.</p>
    <pre><code class="language-python">selected_indices = np.random.choice(embeddings.shape[0], size=subset_size)</code></pre>

    <h3 id="stochastic-greedy">Stochastic Greedy Selection</h3>
    <p>This method iteratively builds the subset by sampling a candidate pool \( R_t \) and selecting the best candidate based on a marginal gain criterion. It supports two objective types:</p>
    <ul>
      <li><strong>Facility Location:</strong> Maximize similarity coverage over the dataset.</li>
      <li><strong>Sum of Squared Norms:</strong> Similar to CRAIG, but with greedy sampling.</li>
    </ul>
    <p>Gain for Facility Location is given by:</p>
    <p>
      \[
      \text{Gain}(x) = \sum_{j \in D} \max(0, \text{sim}(x, j) - \max_{s \in S} \text{sim}(s, j))
      \]
    </p>
    <p>It is efficient and empirically effective for large-scale selections.</p>

    <h3 id="comparison">Comparison Table</h3>
    <table>
      <thead>
        <tr>
          <th>Method</th>
          <th>Objective</th>
          <th>Optimizes</th>
          <th>Complexity</th>
          <th>Strength</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>CRAIG</td>
          <td>\( \text{Tr}(G^T G) \)</td>
          <td>L2 Norm</td>
          <td>\( O(n \log k) \)</td>
          <td>Fast & simple</td>
        </tr>
        <tr>
          <td>PBC</td>
          <td>Bilevel Optimization</td>
          <td>Expected Validation Loss</td>
          <td>\( O(nm) + \text{training loop} \)</td>
          <td>Theoretically optimal</td>
        </tr>
        <tr>
          <td>Stochastic Greedy</td>
          <td>Greedy Marginal Gain</td>
          <td>Similarity / Norm Gain</td>
          <td>\( O(kR) \)</td>
          <td>Balanced & scalable</td>
        </tr>
      </tbody>
    </table>

    <h3 id="code">Subset Selection Code</h3>
    <p>The full pipeline loads embeddings, selects indices using the chosen method, loads the dataset, and saves the selected data subset to a JSON file.</p>
    <pre><code class="language-python">parser.add_argument("--method", choices=["craig", "pbc", "stochastic_greedy"])
if args.method == "craig":
    select_subset_craig(...)
elif args.method == "pbc":
    select_subset_pbc(...)
elif args.method == "stochastic_greedy":
    select_subset_stochastic_greedy(...)</code></pre>

    <p><strong>For results you can refer to the <a href="https://github.com/cv-chaitali/AlgoData.git">https://github.com/cv-chaitali/AlgoData.git</a></strong></p>
  </main>
</body>
</html>
